{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OSL Batch Utils (currently error with extract_fiducials_from_fif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from glob import glob\n",
    "from dask.distributed import Client\n",
    "import coinsmeg_data as coinsmeg\n",
    "from osl import source_recon, utils\n",
    "import numpy as np\n",
    "\n",
    "# Directories\n",
    "BASE_DIR = '/ohba/pi/lhunt/datasets/coins-meg_data'\n",
    "DATA_DIR = coinsmeg.RAW_DIR # this is the same as BASE_DIR as raw data is stored in the base directory\n",
    "PREPROC_DIR = coinsmeg.PREPROCESSED_DIR\n",
    "PREPROC_FILE = PREPROC_DIR + \"/sub-{0}_ses-2-meg_task-coinsmeg_run-{1}_meg_transsss/sub-{0}_ses-2-meg_task-coinsmeg_run-{1}_meg_transsss_preproc_raw.fif\"\n",
    "\n",
    "PREPROC_OUT_TEST = PREPROC_DIR + \"/preprocessed-test/\"\n",
    "\n",
    "ANAT_DIR = coinsmeg.get_sub_anat_dir(\"sub-{0}\")\n",
    "SMRI_FILE = ANAT_DIR + \"/sub-{0}_T1w.nii\"\n",
    "\n",
    "FSL_DIR = \"/home/ali/fsl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define needed functions\n",
    "def fix_headshape_points(outdir, subject):\n",
    "    filenames = source_recon.rhino.get_coreg_filenames(outdir, subject)\n",
    "\n",
    "    # Load saved headshape and nasion files\n",
    "    hs = np.loadtxt(filenames[\"polhemus_headshape_file\"])\n",
    "    nas = np.loadtxt(filenames[\"polhemus_nasion_file\"])\n",
    "    lpa = np.loadtxt(filenames[\"polhemus_lpa_file\"])\n",
    "    rpa = np.loadtxt(filenames[\"polhemus_rpa_file\"])\n",
    "\n",
    "    # Remove headshape points on the nose\n",
    "    remove = np.logical_and(hs[1] > max(lpa[1], rpa[1]), hs[2] < nas[2])\n",
    "    hs = hs[:, ~remove]\n",
    "\n",
    "    # Overwrite headshape file\n",
    "    utils.logger.log_or_print(f\"overwritting {filenames['polhemus_headshape_file']}\")\n",
    "    np.savetxt(filenames[\"polhemus_headshape_file\"], hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OSL osl_logger: handler 'console' level set to 'INFO'\n",
      " OSL Logger Started\n"
     ]
    }
   ],
   "source": [
    "utils.logger.set_up(level=\"INFO\")\n",
    "source_recon.setup_fsl(FSL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subjects\n",
    "subs = []\n",
    "\n",
    "data_sub_folders = sorted(filter(lambda path: \"sub-\" in path, glob(DATA_DIR + '/*'))) # returns a list of paths e.g., '/ohba/pi/lhunt/datasets/coins-meg_data/sub-22'\n",
    "\n",
    "for subject in data_sub_folders:\n",
    "    subs.append(pathlib.Path(subject).stem) # subs is now a list of ['sub-01', 'sub-02', ...]\n",
    "\n",
    "# Setup files\n",
    "smri_files = []\n",
    "preproc_files = []\n",
    "subjects = []\n",
    "for subject in subs:\n",
    "    sub = subject.split('-')[1] # e.g., '01'\n",
    "    mri_file = SMRI_FILE.format(sub) # e.g., '/ohba/pi/lhunt/datasets/coins-meg_data/sub-01/ses-3-structural/anatsub-01_T1w.nii'\n",
    "    if not os.path.exists(mri_file): # if the smri file for subject, specified by mri_file, does not actually exist...\n",
    "        continue\n",
    "    else:\n",
    "        smri_files.append(mri_file) # add the smri filepath to the list smri_files\n",
    "    preproc_file = glob(PREPROC_FILE.replace('{0}', sub).replace('{1}', '*')) # list of 4 ...transsss_preproc_raw.fif files\n",
    "    # maybe exclude sub 21 for now as they have 5 fif runs\n",
    "    for f in preproc_file:\n",
    "        subjects.append(f\"{subject}_run-{f.split('run-')[1][0]}\")\n",
    "        preproc_files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-04_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-04_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-04_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-04_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-04_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-04_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-04_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-04_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-07_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-07_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-07_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-07_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-07_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-07_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-07_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-07_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-09_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-09_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-09_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-09_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-09_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-09_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-09_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-09_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-10_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-10_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-10_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-10_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-10_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-10_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-10_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-10_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-11_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-11_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-11_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-11_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-11_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-11_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-11_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-11_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-12_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-12_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-12_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-12_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-12_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-12_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-12_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-12_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-13_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-13_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-13_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-13_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-13_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-13_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-13_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-13_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-14_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-14_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-14_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-14_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-14_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-14_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-14_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-14_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-15_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-15_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-15_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-15_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-15_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-15_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-15_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-15_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-16_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-16_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-16_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-16_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-16_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-16_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-16_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-16_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-17_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-17_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-17_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-17_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-17_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-17_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-17_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-17_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-18_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-18_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-18_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-18_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-18_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-18_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-18_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-18_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-20_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-20_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-20_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-20_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-20_ses-2-meg_task-coinsmeg_run-1_meg_transsss/sub-20_ses-2-meg_task-coinsmeg_run-1_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-20_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-20_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-21_ses-2-meg_task-coinsmeg_run-3_meg_transsss/sub-21_ses-2-meg_task-coinsmeg_run-3_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-21_ses-2-meg_task-coinsmeg_run-2_meg_transsss/sub-21_ses-2-meg_task-coinsmeg_run-2_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-21_ses-2-meg_task-coinsmeg_run-4_meg_transsss/sub-21_ses-2-meg_task-coinsmeg_run-4_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-21_ses-2-meg_task-coinsmeg_run-1-2nd-60hz_meg_transsss/sub-21_ses-2-meg_task-coinsmeg_run-1-2nd-60hz_meg_transsss_preproc_raw.fif', '/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/sub-21_ses-2-meg_task-coinsmeg_run-1-1st-120hz_meg_transsss/sub-21_ses-2-meg_task-coinsmeg_run-1-1st-120hz_meg_transsss_preproc_raw.fif']\n"
     ]
    }
   ],
   "source": [
    "print(preproc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/ohba/pi/lhunt/datasets/coins-meg_data/sub-04/ses-3-structural/anat/sub-04_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-05/ses-3-structural/anat/sub-05_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-06/ses-3-structural/anat/sub-06_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-07/ses-3-structural/anat/sub-07_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-09/ses-3-structural/anat/sub-09_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-10/ses-3-structural/anat/sub-10_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-11/ses-3-structural/anat/sub-11_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-12/ses-3-structural/anat/sub-12_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-13/ses-3-structural/anat/sub-13_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-14/ses-3-structural/anat/sub-14_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-15/ses-3-structural/anat/sub-15_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-16/ses-3-structural/anat/sub-16_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-17/ses-3-structural/anat/sub-17_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-18/ses-3-structural/anat/sub-18_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-19/ses-3-structural/anat/sub-19_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-20/ses-3-structural/anat/sub-20_T1w.nii', '/ohba/pi/lhunt/datasets/coins-meg_data/sub-21/ses-3-structural/anat/sub-21_T1w.nii']\n"
     ]
    }
   ],
   "source": [
    "print(smri_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OSL osl_logger: handler 'console' level set to 'INFO'\n",
      " OSL Logger Started\n",
      " OSL osl_logger: handler 'console' level set to 'INFO'\n",
      " logging to file: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/logs/osl_batch.log\n",
      " Starting OSL Batch Source Reconstruction\n",
      " Running config\n",
      " {'source_recon': [{'extract_fiducials_from_fif': {}},\n",
      "                  {'fix_headshape_points': {}},\n",
      "                  {'compute_surfaces': {'include_nose': True}},\n",
      "                  {'coregister': {'use_headshape': True, 'use_nose': True}},\n",
      "                  {'forward_model': {'model': 'Single Layer'}},\n",
      "                  {'beamform_and_parcellate': {'chantypes': ['mag', 'grad'],\n",
      "                                               'extra_chans': ['eog', 'ecg'],\n",
      "                                               'freq_range': [1, 80],\n",
      "                                               'method': 'spatial_basis',\n",
      "                                               'orthogonalisation': 'symmetric',\n",
      "                                               'parcellation_file': 'HarvOxf-sub-Schaefer100-combined-2mm_4d_ds8.nii.gz',\n",
      "                                               'rank': {'meg': 60}}}]}\n",
      " Dask Client : <Client: 'tcp://127.0.0.1:33631' processes=8 threads=8, memory=134.35 GB>\n",
      " Dask Client dashboard link: \n",
      " Function defined : functools.partial(<function run_src_chain at 0x7ff170fe1d30>, logsdir=PosixPath('/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/logs'), reportdir=PosixPath('/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/src_report'), gen_report=True, verbose='INFO', mneverbose='WARNING', extra_funcs=[<function fix_headshape_points at 0x7ff1f0c92d30>])\n",
      "sub-04_run-2 : OSL osl_logger: handler 'console' level set to 'INFO'\n",
      "sub-04_run-2 : logging to file: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/logs/sub-04_run-2_src.log\n",
      "sub-04_run-2 : 2024-10-26 17:37:07 : Starting OSL Processing\n",
      "sub-04_run-2 : input : /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/sub-04_run-2\n",
      "sub-04_run-2 : *********************************\n",
      "sub-04_run-2 : * SOURCE RECONSTRUCTION FAILED! *\n",
      "sub-04_run-2 : *********************************\n",
      "sub-04_run-2 : extract_fiducials_from_fif : <function extract_fiducials_from_fif at 0x7f3dca825310>\n",
      "sub-04_run-2 : <class 'TypeError'>\n",
      "sub-04_run-2 : object of type 'NoneType' has no len()\n",
      "sub-04_run-2 : None\n",
      "sub-04_run-1 : OSL osl_logger: handler 'console' level set to 'INFO'\n",
      "sub-04_run-1 : logging to file: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/logs/sub-04_run-1_src.log\n",
      "sub-04_run-1 : 2024-10-26 17:37:07 : Starting OSL Processing\n",
      "sub-04_run-1 : input : /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/sub-04_run-1\n",
      "sub-04_run-1 : *********************************\n",
      "sub-04_run-1 : * SOURCE RECONSTRUCTION FAILED! *\n",
      "sub-04_run-1 : *********************************\n",
      "sub-04_run-1 : extract_fiducials_from_fif : <function extract_fiducials_from_fif at 0x7f84abcc6820>\n",
      "sub-04_run-1 : <class 'TypeError'>\n",
      "sub-04_run-1 : object of type 'NoneType' has no len()\n",
      "sub-04_run-1 : None\n",
      "sub-04_run-4 : OSL osl_logger: handler 'console' level set to 'INFO'\n",
      "sub-04_run-4 : logging to file: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/logs/sub-04_run-4_src.log\n",
      "sub-04_run-4 : 2024-10-26 17:37:07 : Starting OSL Processing\n",
      "sub-04_run-4 : input : /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/sub-04_run-4\n",
      "sub-04_run-4 : *********************************\n",
      "sub-04_run-4 : * SOURCE RECONSTRUCTION FAILED! *\n",
      "sub-04_run-4 : *********************************\n",
      "sub-04_run-4 : extract_fiducials_from_fif : <function extract_fiducials_from_fif at 0x7f53bda0fc10>\n",
      "sub-04_run-4 : <class 'TypeError'>\n",
      "sub-04_run-4 : object of type 'NoneType' has no len()\n",
      "sub-04_run-4 : None\n",
      "sub-04_run-3 : OSL osl_logger: handler 'console' level set to 'INFO'\n",
      "sub-04_run-3 : logging to file: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/logs/sub-04_run-3_src.log\n",
      "sub-04_run-3 : 2024-10-26 17:37:07 : Starting OSL Processing\n",
      "sub-04_run-3 : input : /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/sub-04_run-3\n",
      "sub-04_run-3 : *********************************\n",
      "sub-04_run-3 : * SOURCE RECONSTRUCTION FAILED! *\n",
      "sub-04_run-3 : *********************************\n",
      "sub-04_run-3 : extract_fiducials_from_fif : <function extract_fiducials_from_fif at 0x7fe2f1e55940>\n",
      "sub-04_run-3 : <class 'TypeError'>\n",
      "sub-04_run-3 : object of type 'NoneType' has no len()\n",
      "sub-04_run-3 : None\n",
      " Computation complete\n",
      " OSL osl_logger: handler 'console' level set to 'INFO'\n",
      " logging to file: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/preprocessed/preprocessed-test/logs/osl_batch.log\n",
      " Processed 0/4 files successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/Desktop/osl/osl/source_recon/batch.py:153: RuntimeWarning: Log entries will be appended to the file. Use overwrite=False to avoid this message in the future.\n",
      "  mne.utils._logging.set_log_file(logfile)\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 199, in run_src_chain\n",
      "    wrapped_func(\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 191, in wrapped_func\n",
      "    args_with_defaults = args[-len(defaults):]\n",
      "/home/ali/Desktop/osl/osl/source_recon/batch.py:153: RuntimeWarning: Log entries will be appended to the file. Use overwrite=False to avoid this message in the future.\n",
      "  mne.utils._logging.set_log_file(logfile)\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 199, in run_src_chain\n",
      "    wrapped_func(\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 191, in wrapped_func\n",
      "    args_with_defaults = args[-len(defaults):]\n",
      "/home/ali/Desktop/osl/osl/source_recon/batch.py:153: RuntimeWarning: Log entries will be appended to the file. Use overwrite=False to avoid this message in the future.\n",
      "  mne.utils._logging.set_log_file(logfile)\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 199, in run_src_chain\n",
      "    wrapped_func(\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 191, in wrapped_func\n",
      "    args_with_defaults = args[-len(defaults):]\n",
      "/home/ali/Desktop/osl/osl/source_recon/batch.py:153: RuntimeWarning: Log entries will be appended to the file. Use overwrite=False to avoid this message in the future.\n",
      "  mne.utils._logging.set_log_file(logfile)\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 199, in run_src_chain\n",
      "    wrapped_func(\n",
      "  File \"/home/ali/Desktop/osl/osl/source_recon/batch.py\", line 191, in wrapped_func\n",
      "    args_with_defaults = args[-len(defaults):]\n"
     ]
    }
   ],
   "source": [
    "# test case on two subjects\n",
    "if __name__ == \"__main__\":\n",
    "    utils.logger.set_up(level=\"INFO\")\n",
    "    source_recon.setup_fsl(FSL_DIR)\n",
    "\n",
    "    # Get subjects\n",
    "    #subs = []\n",
    "    #for subject in sorted(glob(ANAT_DIR + '/*')):\n",
    "    #    subs.append(pathlib.Path(subject).stem)\n",
    "\n",
    "    subs = ['sub-04']\n",
    "    \n",
    "    # Setup files\n",
    "    smri_files = []\n",
    "    preproc_files = []\n",
    "    subjects = []\n",
    "    for subject in subs:\n",
    "        sub = subject.split('-')[1] # e.g., '01'\n",
    "        mri_file = SMRI_FILE.format(sub) # e.g., '/ohba/pi/lhunt/datasets/coins-meg_data/sub-01/ses-3-structural/anatsub-01_T1w.nii'\n",
    "        if not os.path.exists(mri_file): # if the smri file for subject, specified by mri_file, does not actually exist...\n",
    "            continue\n",
    "        preproc_file = glob(PREPROC_FILE.replace('{0}', sub).replace('{1}', '*')) # list of 4 ...transsss_preproc_raw.fif files\n",
    "        # maybe exclude sub 21 for now as they have 5 fif runs\n",
    "        for f in preproc_file:\n",
    "            subjects.append(f\"{subject}_run-{f.split('run-')[1][0]}\")\n",
    "            preproc_files.append(f)\n",
    "            smri_files.append(mri_file) # add the smri filepath to the list smri_files\n",
    "\n",
    "    # Settings\n",
    "    config = \"\"\"\n",
    "        source_recon:\n",
    "        - extract_fiducials_from_fif: {}\n",
    "        - fix_headshape_points: {}\n",
    "        - compute_surfaces:\n",
    "            include_nose: True\n",
    "        - coregister:\n",
    "            use_nose: True\n",
    "            use_headshape: True\n",
    "        - forward_model:\n",
    "            model: Single Layer\n",
    "        - beamform_and_parcellate:\n",
    "            freq_range: [1, 80]\n",
    "            chantypes: [mag, grad]\n",
    "            rank: {meg: 60}\n",
    "            parcellation_file: HarvOxf-sub-Schaefer100-combined-2mm_4d_ds8.nii.gz\n",
    "            method: spatial_basis\n",
    "            orthogonalisation: symmetric\n",
    "            extra_chans: [eog, ecg]\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup parallel processing\n",
    "    client = Client(n_workers=8, threads_per_worker=1)\n",
    "\n",
    "    # Run beamforming and parcellation\n",
    "    source_recon.run_src_batch(\n",
    "        config,\n",
    "        outdir=PREPROC_OUT_TEST,\n",
    "        subjects=subjects,\n",
    "        preproc_files=preproc_files,\n",
    "        smri_files=smri_files,\n",
    "        extra_funcs=[fix_headshape_points],\n",
    "        dask_client=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT Using OSL Batch Utils -- This is working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import os.path as op\n",
    "import osl\n",
    "from pprint import pprint\n",
    "from osl import utils\n",
    "from osl import source_recon\n",
    "import numpy as np\n",
    "import coinsmeg_data as coinsmeg\n",
    "import open3d\n",
    "from IPython.display import HTML, display\n",
    "import mne\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def copy_polhemus_files(recon_dir, subject, preproc_file, smri_file, logger):\n",
    "    polhemus_headshape = np.loadtxt(op.join(polhemus_dir, 'polhemus_headshape.txt'))\n",
    "    polhemus_nasion = np.loadtxt(op.join(polhemus_dir, 'polhemus_nasion.txt'))\n",
    "    polhemus_rpa = np.loadtxt(op.join(polhemus_dir, 'polhemus_rpa.txt'))\n",
    "    polhemus_lpa = np.loadtxt(op.join(polhemus_dir, 'polhemus_lpa.txt'))\n",
    "    \n",
    "    # Â Get coreg filenames\n",
    "    filenames = source_recon.rhino.get_coreg_filenames(recon_dir, subject)\n",
    "\n",
    "    # Save\n",
    "    np.savetxt(filenames[\"polhemus_nasion_file\"], polhemus_nasion)\n",
    "    np.savetxt(filenames[\"polhemus_rpa_file\"], polhemus_rpa)\n",
    "    np.savetxt(filenames[\"polhemus_lpa_file\"], polhemus_lpa)\n",
    "    np.savetxt(filenames[\"polhemus_headshape_file\"], polhemus_headshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "preproc_dir = op.join(coinsmeg.DERIVATIVES_DIR, \"preprocessed\")\n",
    "recon_dir = op.join(coinsmeg.DERIVATIVES_DIR, \"recon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-01_run-1', 'sub-01_run-2', 'sub-01_run-3', 'sub-01_run-4', 'sub-02_run-1', 'sub-02_run-2', 'sub-02_run-3', 'sub-02_run-4', 'sub-03_run-1', 'sub-03_run-2', 'sub-03_run-3', 'sub-03_run-4', 'sub-04_run-1', 'sub-04_run-2', 'sub-04_run-3', 'sub-04_run-4', 'sub-05_run-1', 'sub-05_run-2', 'sub-05_run-3', 'sub-05_run-4', 'sub-06_run-1', 'sub-06_run-2', 'sub-06_run-3', 'sub-06_run-4', 'sub-07_run-1', 'sub-07_run-2', 'sub-07_run-3', 'sub-07_run-4', 'sub-08_run-1', 'sub-08_run-2', 'sub-08_run-3', 'sub-08_run-4', 'sub-09_run-1', 'sub-09_run-2', 'sub-09_run-3', 'sub-09_run-4', 'sub-10_run-1', 'sub-10_run-2', 'sub-10_run-3', 'sub-10_run-4', 'sub-11_run-1', 'sub-11_run-2', 'sub-11_run-3', 'sub-11_run-4', 'sub-12_run-1', 'sub-12_run-2', 'sub-12_run-3', 'sub-12_run-4', 'sub-13_run-1', 'sub-13_run-2', 'sub-13_run-3', 'sub-13_run-4', 'sub-14_run-1', 'sub-14_run-2', 'sub-14_run-3', 'sub-14_run-4', 'sub-15_run-1', 'sub-15_run-2', 'sub-15_run-3', 'sub-15_run-4', 'sub-16_run-1', 'sub-16_run-2', 'sub-16_run-3', 'sub-16_run-4', 'sub-17_run-1', 'sub-17_run-2', 'sub-17_run-3', 'sub-17_run-4', 'sub-18_run-1', 'sub-18_run-2', 'sub-18_run-3', 'sub-18_run-4', 'sub-19_run-1', 'sub-19_run-2', 'sub-19_run-3', 'sub-19_run-4', 'sub-20_run-1', 'sub-20_run-2', 'sub-20_run-3', 'sub-20_run-4', 'sub-21_run-1', 'sub-21_run-2', 'sub-21_run-3', 'sub-21_run-4', 'sub-22_run-1', 'sub-22_run-2', 'sub-22_run-3', 'sub-22_run-4']\n"
     ]
    }
   ],
   "source": [
    "# Get subjects\n",
    "subs = []\n",
    "\n",
    "data_sub_folders = sorted(filter(lambda path: \"sub-\" in path, glob(DATA_DIR + '/*'))) # returns a list of paths e.g., '/ohba/pi/lhunt/datasets/coins-meg_data/sub-22'\n",
    "\n",
    "for subject in data_sub_folders:\n",
    "    subs.append(pathlib.Path(subject).stem) # subs is now a list of ['sub-01', 'sub-02', ...]\n",
    "\n",
    "# Create a list with '_run-X' appended for each subject\n",
    "sub_run_combos = []\n",
    "for sub in subs:\n",
    "    for run in range(1, 5):  # Loop from 1 to 4\n",
    "        sub_run_combos.append(f\"{sub}_run-{run}\") # sub_run_combos is now a list of ['sub-01_run-1', 'sub-01_run-2', ...]\n",
    "\n",
    "print(sub_run_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with only two sub/run combos\n",
    "sub_run_combos =['sub-04_run-1','sub-04_run-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** RUNNING OSL RHINO COMPUTE SURFACES ***\n",
      " The nose is going to be added to the outer skin (scalp) surface.\n",
      " Please ensure that the structural MRI has a FOV that includes the nose\n",
      " reorienting subject brain to be RADIOLOGICAL\n",
      " You can use the following call to check the passed in structural MRI is appropriate,\n",
      " including checking that the L-R, S-I, A-P labels are sensible:\n",
      " In Python:\n",
      " fsleyes(\"/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/surfaces/smri.nii.gz\", \"/home/ali/fsl/data/standard/MNI152_T1_1mm_brain.nii.gz\")\n",
      " From the cmd line:\n",
      " fsleyes /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/surfaces/smri.nii.gz /home/ali/fsl/data/standard/MNI152_T1_1mm_brain.nii.gz\n",
      " Running BET pre-FLIRT...\n",
      " Running FLIRT...\n",
      " Running BET and BETSURF...\n",
      " Refining scalp surface...\n",
      " Adding nose to scalp surface...\n",
      " rhino.surfaces.surfaces_display(\"/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon\", \"sub-04_run-1\") can be used to check the result\n",
      " *** OSL RHINO COMPUTE SURFACES COMPLETE ***\n",
      " Extracting polhemus from fif info\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/polhemus/polhemus_nasion.txt\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/polhemus/polhemus_rpa.txt\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/polhemus/polhemus_lpa.txt\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/polhemus/polhemus_headshape.txt\n",
      " WARNING: fif filename contains '_trans' which suggests -trans was passed during MaxFiltering\n",
      " WARNING: This means the location of the head in the coregistration plot may not be correct\n",
      " WARNING: Either use the _tsss.fif file or ignore the centroid of the head in coregistration plot\n",
      " *** RUNNING OSL RHINO COREGISTRATION ***\n",
      " The MRI-derived nose is going to be used to aid coreg.\n",
      " Please ensure that rhino.compute_surfaces was run with include_nose=True.\n",
      " Please ensure that the polhemus headshape points include the nose.\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/coreg/polhemus_headshape.txt\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/coreg/polhemus_nasion.txt\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/coreg/polhemus_rpa.txt\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/coreg/polhemus_lpa.txt\n",
      " Using known MNI fiducials\n",
      " Running ICP...\n",
      " ICP found better xform, error=2.00207692132556\n",
      " rhino.coreg_display(\"/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon\", \"sub-04_run-1\") can be used to check the result\n",
      " *** OSL RHINO COREGISTRATION COMPLETE ***\n",
      "Creating surface mesh for /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/coreg/scaled_outskin_plus_nose_mesh.nii.gz .....\n",
      " saving /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/coreg/coreg_display_plot.html\n"
     ]
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.1.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1;\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n  var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'vtk': 'https://cdn.jsdelivr.net/npm/vtk.js@20.0.1/vtk', 'jspanel': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/jspanel', 'jspanel-modal': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal', 'jspanel-tooltip': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip', 'jspanel-hint': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint', 'jspanel-layout': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout', 'jspanel-contextmenu': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu', 'jspanel-dock': 'https://cdn.jsdelivr.net/npm/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock', 'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@7.2.3/dist/gridstack-all', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'vtk': {'exports': 'vtk'}, 'jspanel': {'exports': 'jsPanel'}, 'gridstack': {'exports': 'GridStack'}}});\n      require([\"vtk\"], function() {\n\ton_load()\n      })\n      require([\"jspanel\"], function(jsPanel) {\n\twindow.jsPanel = jsPanel\n\ton_load()\n      })\n      require([\"jspanel-modal\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-tooltip\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-hint\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-layout\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-contextmenu\"], function() {\n\ton_load()\n      })\n      require([\"jspanel-dock\"], function() {\n\ton_load()\n      })\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 10;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window['vtk'] !== undefined) && (!(window['vtk'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/abstractvtkplot/vtk.js@20.0.1/vtk.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['jsPanel'] !== undefined) && (!(window['jsPanel'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/jspanel.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/modal/jspanel.modal.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/tooltip/jspanel.tooltip.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/hint/jspanel.hint.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/layout/jspanel.layout.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/contextmenu/jspanel.contextmenu.js', 'https://cdn.holoviz.org/panel/1.2.3/dist/bundled/floatpanel/jspanel4@4.12.0/dist/extensions/dock/jspanel.dock.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/gridstack/gridstack@7.2.3/dist/gridstack-all.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.2.3/dist/bundled/notificationarea/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.holoviz.org/panel/1.2.3/dist/bundled/abstractvtkplot/vtk.js@20.0.1/vtk.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.1.1.min.js\", \"https://cdn.holoviz.org/panel/1.2.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      Bokeh = root.Bokeh;\n      bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      if (!reloading && (!bokeh_loaded || is_dev)) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='bac2497e-f9ba-461b-9894-dfb235677c26'>\n",
       "  <div id=\"d1ae0066-493c-4138-9a98-df1b583fdf50\" data-root-id=\"bac2497e-f9ba-461b-9894-dfb235677c26\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"032deaa4-c42e-444e-83ee-1e49f1e0bdbe\":{\"version\":\"3.1.1\",\"title\":\"Bokeh Application\",\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]}],\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"bac2497e-f9ba-461b-9894-dfb235677c26\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"56472186-0be1-45c4-a0db-b23ead014fe0\",\"attributes\":{\"plot_id\":\"bac2497e-f9ba-461b-9894-dfb235677c26\",\"comm_id\":\"c6cd2531b0294e67bfca40bcd5c1270d\",\"client_comm_id\":\"5ba92abada5340b7a48da0b675f45551\"}}],\"callbacks\":{\"type\":\"map\"}}};\n",
       "  var render_items = [{\"docid\":\"032deaa4-c42e-444e-83ee-1e49f1e0bdbe\",\"roots\":{\"bac2497e-f9ba-461b-9894-dfb235677c26\":\"d1ae0066-493c-4138-9a98-df1b583fdf50\"},\"root_ids\":[\"bac2497e-f9ba-461b-9894-dfb235677c26\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  const is_dev = py_version.indexOf(\"+\") !== -1 || py_version.indexOf(\"-\") !== -1\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version && !is_dev) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root['vtk'] !== undefined) && ( root['vtk'] !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "bac2497e-f9ba-461b-9894-dfb235677c26"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " *** RUNNING OSL RHINO FORWARD MODEL ***\n",
      " Using bet_inskull_surf_file for single shell surface\n",
      " *** OSL RHINO FORWARD MODEL COMPLETE ***\n",
      " BEM surface: number of dipoles = 1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/envs/osl/lib/python3.8/site-packages/pyvista/plotting/renderer.py:536: UserWarning: VTK compiled with OSMesa does not properly support FXAA anti-aliasing and SSAA will be used instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " saving /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/coreg/bem_display_plot.html\n",
      "/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-1/rhino/model-fwd.fif\n",
      "Leadfield size : 306 sensors x 3666 dipoles\n",
      "Temporal Filtering\n",
      "Completed\n",
      " *** RUNNING OSL MAKE LCMV ***\n",
      " variance for chantype grad is 1.705213409394786e-23\n",
      " variance for chantype mag is 3.780413292790151e-26\n",
      " *** OSL MAKE LCMV COMPLETE ***\n",
      "Applying beamformer spatial filters\n",
      " beamforming.apply_lcmv\n",
      " spatial_resolution = 10 mm\n",
      "Completed\n",
      "Dimensions of reconstructed timeseries in MNI space is (dipoles x all_tpts) = (2527, 794101)\n",
      "Parcellating data\n",
      " gridstep = 10 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: An input intended to be a single 3D volume has multiple timepoints. Input will be truncated to first volume, but this functionality is deprecated and will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Dimensions of parcel timeseries in MNI space is (nparcels x all_tpts) = (114, 794101)\n",
      "Dimensions of parc_raw are (nparcels x all_tpts) = (115, 853000)\n",
      " *** RUNNING OSL RHINO COMPUTE SURFACES ***\n",
      " The nose is going to be added to the outer skin (scalp) surface.\n",
      " Please ensure that the structural MRI has a FOV that includes the nose\n",
      " reorienting subject brain to be RADIOLOGICAL\n",
      " You can use the following call to check the passed in structural MRI is appropriate,\n",
      " including checking that the L-R, S-I, A-P labels are sensible:\n",
      " In Python:\n",
      " fsleyes(\"/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/surfaces/smri.nii.gz\", \"/home/ali/fsl/data/standard/MNI152_T1_1mm_brain.nii.gz\")\n",
      " From the cmd line:\n",
      " fsleyes /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/surfaces/smri.nii.gz /home/ali/fsl/data/standard/MNI152_T1_1mm_brain.nii.gz\n",
      " Running BET pre-FLIRT...\n",
      " Running FLIRT...\n",
      " Running BET and BETSURF...\n",
      " Refining scalp surface...\n",
      " Adding nose to scalp surface...\n",
      " rhino.surfaces.surfaces_display(\"/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon\", \"sub-04_run-2\") can be used to check the result\n",
      " *** OSL RHINO COMPUTE SURFACES COMPLETE ***\n",
      " Extracting polhemus from fif info\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/polhemus/polhemus_nasion.txt\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/polhemus/polhemus_rpa.txt\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/polhemus/polhemus_lpa.txt\n",
      " saved: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/polhemus/polhemus_headshape.txt\n",
      " WARNING: fif filename contains '_trans' which suggests -trans was passed during MaxFiltering\n",
      " WARNING: This means the location of the head in the coregistration plot may not be correct\n",
      " WARNING: Either use the _tsss.fif file or ignore the centroid of the head in coregistration plot\n",
      " *** RUNNING OSL RHINO COREGISTRATION ***\n",
      " The MRI-derived nose is going to be used to aid coreg.\n",
      " Please ensure that rhino.compute_surfaces was run with include_nose=True.\n",
      " Please ensure that the polhemus headshape points include the nose.\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/coreg/polhemus_headshape.txt\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/coreg/polhemus_nasion.txt\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/coreg/polhemus_rpa.txt\n",
      " loading: /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/coreg/polhemus_lpa.txt\n",
      " Using known MNI fiducials\n",
      " Running ICP...\n",
      " ICP found better xform, error=2.00207692132556\n",
      " rhino.coreg_display(\"/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon\", \"sub-04_run-2\") can be used to check the result\n",
      " *** OSL RHINO COREGISTRATION COMPLETE ***\n",
      "Creating surface mesh for /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/coreg/scaled_outskin_plus_nose_mesh.nii.gz .....\n",
      " saving /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/coreg/coreg_display_plot.html\n",
      " *** RUNNING OSL RHINO FORWARD MODEL ***\n",
      " Using bet_inskull_surf_file for single shell surface\n",
      " *** OSL RHINO FORWARD MODEL COMPLETE ***\n",
      " BEM surface: number of dipoles = 1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/anaconda3/envs/osl/lib/python3.8/site-packages/pyvista/plotting/renderer.py:536: UserWarning: VTK compiled with OSMesa does not properly support FXAA anti-aliasing and SSAA will be used instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " saving /ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/coreg/bem_display_plot.html\n",
      "/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon/sub-04_run-2/rhino/model-fwd.fif\n",
      "Leadfield size : 306 sensors x 3666 dipoles\n",
      "Temporal Filtering\n",
      "Completed\n",
      " *** RUNNING OSL MAKE LCMV ***\n",
      " variance for chantype grad is 1.945768377782321e-23\n",
      " variance for chantype mag is 4.207976302326424e-26\n",
      " *** OSL MAKE LCMV COMPLETE ***\n",
      "Applying beamformer spatial filters\n",
      " beamforming.apply_lcmv\n",
      " spatial_resolution = 10 mm\n",
      "Completed\n",
      "Dimensions of reconstructed timeseries in MNI space is (dipoles x all_tpts) = (2527, 740501)\n",
      "Parcellating data\n",
      " gridstep = 10 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: An input intended to be a single 3D volume has multiple timepoints. Input will be truncated to first volume, but this functionality is deprecated and will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Dimensions of parcel timeseries in MNI space is (nparcels x all_tpts) = (114, 740501)\n",
      "Dimensions of parc_raw are (nparcels x all_tpts) = (115, 800000)\n"
     ]
    }
   ],
   "source": [
    "for sub_run_combo in sub_run_combos:\n",
    "\n",
    "    # extract the subject and run id\n",
    "    subject_id, run_id = sub_run_combo.split('_') # will produce subject_id = 'sub-XX', run_id = 'run-X'\n",
    "    # set paths for where to look for necessary files\n",
    "    anat_dir = coinsmeg.get_sub_anat_dir(subject_id)\n",
    "    smri_file = f\"{anat_dir}/{subject_id}_T1w.nii\"\n",
    "    fif_file = f'{preproc_dir}/{subject_id}_ses-2-meg_task-coinsmeg_{run_id}_meg_transsss/{subject_id}_ses-2-meg_task-coinsmeg_{run_id}_meg_transsss_preproc_raw.fif'\n",
    "    \n",
    "    if not os.path.exists(smri_file):\n",
    "        print(f\"WARNING: smri_file does not exist for {sub_run_combo}!\")\n",
    "        continue # skip over the rest of the code for this sub_run_combo\n",
    "\n",
    "    source_recon.rhino.compute_surfaces(\n",
    "        smri_file,\n",
    "        recon_dir,\n",
    "        sub_run_combo,\n",
    "        include_nose=True,\n",
    "    )\n",
    "    # check in fsleyes\n",
    "    source_recon.rhino.surfaces_display(recon_dir, sub_run_combo)\n",
    "    \n",
    "    polhemus_dir = op.join(recon_dir, sub_run_combo, \"polhemus\")\n",
    "    # make directory if it doesn't yet exist\n",
    "    os.makedirs(polhemus_dir, exist_ok=True)\n",
    "\n",
    "    from osl.source_recon.rhino.polhemus import extract_polhemus_from_info\n",
    "\n",
    "    extract_polhemus_from_info(\n",
    "        fif_file = fif_file,\n",
    "        headshape_outfile=op.join(polhemus_dir, \"polhemus_headshape.txt\"),\n",
    "        nasion_outfile=op.join(polhemus_dir, \"polhemus_nasion.txt\"),\n",
    "        rpa_outfile=op.join(polhemus_dir, \"polhemus_rpa.txt\"),\n",
    "        lpa_outfile=op.join(polhemus_dir, \"polhemus_lpa.txt\")\n",
    "    )\n",
    "\n",
    "    # recall that our maxfiltering options were\n",
    "    # f\"--maxpath /neuro/bin/util/maxfilter --mode multistage --scanner Neo --tsss --headpos --movecomp --trans {trans_file}\",)\n",
    "            #  the maxfilter aligns all runs of a participant so that the head position is the same within each participant\n",
    "    \n",
    "    copy_polhemus_files(recon_dir, sub_run_combo, [], [], [])\n",
    "\n",
    "    # Then we run the coreg, for real.\n",
    "\n",
    "    source_recon.rhino.coreg(\n",
    "        fif_file, # full path to the MNE raw fif file\n",
    "        recon_dir, # full path to the directory that contains the subject directories RHINO outputs\n",
    "        sub_run_combo, # the name of the subject directories RHINO outputs to\n",
    "        use_headshape=True,     #use the headshape points to refine the coregistration?\n",
    "        use_nose=True, # use the nose headshape points to refine the coregistration?\n",
    "    )\n",
    "\n",
    "    # now view result\n",
    "    source_recon.rhino.coreg_display(subjects_dir = \"/ohba/pi/lhunt/datasets/coins-meg_data/derivatives/recon\", \n",
    "                                    subject = sub_run_combo,\n",
    "                                    filename = f\"{recon_dir}/{sub_run_combo}/rhino/coreg/coreg_display_plot.html\") # saves an interactive html plot at this location\n",
    "\n",
    "    # Compute forward model\n",
    "    gridstep = 10\n",
    "    source_recon.rhino.forward_model(\n",
    "        recon_dir,\n",
    "        sub_run_combo,\n",
    "        model=\"Single Layer\",\n",
    "        gridstep=gridstep,\n",
    "    )\n",
    "\n",
    "    # view results\n",
    "    source_recon.rhino.bem_display(\n",
    "        recon_dir,\n",
    "        sub_run_combo,\n",
    "        display_outskin_with_nose=False,\n",
    "        display_sensors=True,\n",
    "        plot_type=\"surf\",\n",
    "        filename=f\"{recon_dir}/{sub_run_combo}/rhino/coreg/bem_display_plot.html\",\n",
    "    )\n",
    "\n",
    "    from mne import read_forward_solution\n",
    "\n",
    "    # load forward solution\n",
    "    fwd_fname = op.join(recon_dir, sub_run_combo, \"rhino\", \"model-fwd.fif\") \n",
    "    # tutorial said source_recon.rhino.get_coreg_filenames(recon_dir, subjects[0])[\"forward_model_file\"]\n",
    "    # but this did not return a match for \"forward_model_file\"\n",
    "    print(fwd_fname)\n",
    "\n",
    "    fwd = read_forward_solution(fwd_fname)\n",
    "    leadfield = fwd[\"sol\"][\"data\"]\n",
    "    print(\"Leadfield size : %d sensors x %d dipoles\" % leadfield.shape)\n",
    "\n",
    "    import mne\n",
    "\n",
    "    # Temporal filtering\n",
    "\n",
    "    chantypes = [\"grad\", \"mag\"] \n",
    "\n",
    "    # Get and setup the data\n",
    "    data = mne.io.read_raw_fif(fif_file, preload=True)\n",
    "    data = data.pick(chantypes)\n",
    "\n",
    "    # Filter to the beta band\n",
    "    print(\"Temporal Filtering\")\n",
    "    data = data.filter(\n",
    "        l_freq=1,\n",
    "        h_freq=30,\n",
    "        method=\"iir\",\n",
    "        iir_params={\"order\": 5, \"btype\": \"bandpass\", \"ftype\": \"butter\"},\n",
    "    )\n",
    "    print(\"Completed\")\n",
    "\n",
    "    # Compute BEAMFORMER WEIGHTS\n",
    "    from osl.source_recon import rhino, beamforming, parcellation\n",
    "      \n",
    "    # Make LCMV beamformer filters\n",
    "    # Note that this will exclude any bad time segments when calculating the beamformer filters\n",
    "    filters = beamforming.make_lcmv(\n",
    "        recon_dir,\n",
    "        sub_run_combo,\n",
    "        data,\n",
    "        chantypes,\n",
    "        pick_ori=\"max-power-pre-weight-norm\",\n",
    "        rank={\"meg\": 60},\n",
    "    )\n",
    "\n",
    "    print(\"Applying beamformer spatial filters\")\n",
    "\n",
    "    # stc is source space time series (in head/polhemus space).\n",
    "    stc = beamforming.apply_lcmv(data, filters)\n",
    "\n",
    "    # Convert from head/polhemus space to standard brain grid in MNI space\n",
    "    recon_timeseries_mni, reference_brain_fname, recon_coords_mni, _ = \\\n",
    "            beamforming.transform_recon_timeseries(recon_dir, \n",
    "                                                    sub_run_combo, \n",
    "                                                    recon_timeseries=stc.data, \n",
    "                                                    reference_brain=\"mni\")\n",
    "\n",
    "    print(\"Completed\")\n",
    "    print(\"Dimensions of reconstructed timeseries in MNI space is (dipoles x all_tpts) = {}\".format(recon_timeseries_mni.shape))\n",
    "\n",
    "    # PARCELLATION\n",
    "    parcellation_fname = 'HarvOxf-sub-Schaefer100-combined-2mm_4d_ds8.nii.gz'\n",
    "\n",
    "    print(\"Parcellating data\")\n",
    "\n",
    "    # Apply parcellation to (voxels x all_tpts) data contained in recon_timeseries_mni.\n",
    "    # The resulting parcel_timeseries will be (parcels x all_tpts) in MNI space\n",
    "    # where all_tpts includes bad time segments\n",
    "    parcel_ts, _, _ = parcellation.parcellate_timeseries(\n",
    "        parcellation_fname, # corresponds to the -beamform_and_parcellate: method: parcellation_file\n",
    "        recon_timeseries_mni, # reconstructed timeseries in MNI space; dimensions are (dipoles x all_tpts)\n",
    "        recon_coords_mni,  # dimensions are 3 x dipoles ((3, 2527)). the 3 rows are x, y, and z coordinates in MNI space\n",
    "        \"spatial_basis\",  # corresponds to the -beamform_and_parcellate: method: setting\n",
    "        recon_dir,\n",
    "    )\n",
    "\n",
    "    print(\"Completed\")\n",
    "    print(\"Dimensions of parcel timeseries in MNI space is (nparcels x all_tpts) = {}\".format(parcel_ts.shape))\n",
    "\n",
    "    # Create mne raw object for the parcellated data\n",
    "\n",
    "    # We reload raw data to ensure that the stim channel is in there\n",
    "    raw = mne.io.read_raw_fif(fif_file) # recall that fif_file is a specific subject/run\n",
    "    parc_raw = parcellation.convert2mne_raw(parcel_ts, raw)\n",
    "\n",
    "    print(\"Dimensions of parc_raw are (nparcels x all_tpts) = {}\".format(parc_raw.get_data().shape))\n",
    "\n",
    "    # source space data directory\n",
    "    src_dir = op.join(coinsmeg.DERIVATIVES_DIR, \"src\", sub_run_combo) # directory for saving source reconstructed files\n",
    "    os.makedirs(src_dir, exist_ok=True)\n",
    "\n",
    "    # save parc_raw into the src_dir\n",
    "    parc_raw.save(op.join(src_dir, \"parc_raw.fif\"), overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
